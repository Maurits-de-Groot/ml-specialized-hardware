{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d041fc56-c7b8-456b-ac11-97aef399c9cf",
   "metadata": {},
   "source": [
    "# AWS Machine Learning Purpose-built Accelerators Tutorial\n",
    "## Learn how to use [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/) with [Amazon SageMaker](https://aws.amazon.com/sagemaker/), to optimize your ML workload\n",
    "## Part 2/3 - Finetuning a Bert model with SageMaker + [Hugging Face Optimum Neuron](https://huggingface.co/docs/optimum-neuron/index) on a Trainum instance\n",
    "\n",
    "**SageMaker studio Kernel: PyTorch 1.13 Python 3.9 CPU - ml.t3.medium** \n",
    "\n",
    "In this tutorial, you'll learn how to kick-off a finetuning job on SageMaker, with [HF Optimum Neuron](https://huggingface.co/docs/optimum-neuron/index) on a [trn1 instance\n",
    "](https://aws.amazon.com/ec2/instance-types/trn1/). HF Optimum Neuron is a framework that simplifies the training script and helps ML developers to create a portable code that can be reused in many scenarios, for instance: Different models, different tasks, distributed training (data parallel, tensor parallel, etc.). Also, Optimum Neuron helps you to compile your model and deploy to AWS Inferentia (learn more in the 3rd part of this tutorial). \n",
    "\n",
    "In section 02, you'll see how to extract metadata from the Optimum Neuron API and render a table with the current tested/supported models (similar models not listed there can also be compatible, but you need to check by yourself). This table is important for you to understand which models can be selected and fine-tuned in a simple way. However, before selecting a model for training, check a similar table in the notebook **Part 3** to see which models can be deployed to AWS Inferentia using HF Optimum Neuron. That way you can plan your end2end solution and start implementing it right now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d3e38-3c43-4b4f-af22-26e2e127a0d9",
   "metadata": {},
   "source": [
    "## 1) Install some required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1b8c3-e280-426b-a70b-b4de5b4c575b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U optimum-neuron==0.0.8 onnx>=1.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d435090-f88c-4241-bfa2-1a5bf6c0e39b",
   "metadata": {},
   "source": [
    "## 2) Supported models/tasks for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf36f85b-3416-4be5-b092-fdccfb496f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown\n",
    "from optimum.exporters.tasks import TasksManager\n",
    "from optimum.exporters.neuron.model_configs import *\n",
    "from optimum.neuron.distributed.parallelizers_manager import ParallelizersManager\n",
    "from optimum.neuron.utils.training_utils import (\n",
    "    _SUPPORTED_MODEL_NAMES,\n",
    "    _SUPPORTED_MODEL_TYPES,\n",
    "    _generate_supported_model_class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5880a8-762e-4fcb-95dd-c6ef120be393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve supported models for Tensor Parallelism\n",
    "tp_support = list(ParallelizersManager._MODEL_TYPE_TO_PARALLEL_MODEL_CLASS.keys())\n",
    "\n",
    "# build compability table for training\n",
    "data_training = {'Model': []}\n",
    "for m in _SUPPORTED_MODEL_TYPES:\n",
    "    if type(m) != str: m = m[0]\n",
    "    if m=='gpt-2': m='gpt2' # fix the name\n",
    "    model_id = len(data_training['Model'])\n",
    "    model_link = f'<a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search={m}\">{m}</a>'\n",
    "    data_training['Model'].append(f\"{model_link} <font style='color: red;'><b>[TP]</b></font>\" if m in tp_support else model_link)\n",
    "    tasks = [re.sub(r'.+For(.+)', r'\\1', t) for t in set(_generate_supported_model_class_names(m)) if not t.endswith('Model')]\n",
    "    for t in tasks:\n",
    "        if data_training.get(t) is None: data_training[t] = [''] * len(_SUPPORTED_MODEL_TYPES)\n",
    "        data_training[t][model_id] = f'<a target=\"_new\" href=\"https://huggingface.co/docs/transformers/model_doc/{m}#transformers.{m.title()}For{t}\">api</a>'        \n",
    "df_training = pd.DataFrame.from_dict(data_training).set_index('Model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ccae0-ff63-49f4-a006-6c6012fb65e5",
   "metadata": {},
   "source": [
    "In each new release of HF Optimum Neuron, new models will be included in this list. So, it is expected to see different values for the following tables when you upgrade the library.  \n",
    "\n",
    "Models with **[TP]** after the name support Tensor Parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b57531-836a-4e63-8da9-d508891feee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Markdown(df_training.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2245d70a-bfa2-40a7-82a3-b61bc3577e92",
   "metadata": {},
   "source": [
    "## 3) Fine-tuning a model, using SageMaker and HF Optimum Neuron\n",
    "We're training a Bert model as a text classifier to predict if an input email is SPAM or NOT. To adapt it for your own scenario, just change the following variables: **MODEL** and **TASK** using the table above as a reference.  \n",
    "  - MODEL: name of the model available on the HF portal. Click on the desired \"model name\" in the table above to list all the options for that particular model.\n",
    "  - TASK: copy desired the task (column name) from the table above. Make sure the model you selected supports that particular task, otherwise, you need to change your model.\n",
    "\n",
    "**You need Hugging Face credentials and a custom repo** to run this sample. This configuration is required to store the cache files of your model. Just go to [huggingface.co] (huggingface.co/) and create and account, if needed. You also need to generate an **access token** and a new model repository.\n",
    "\n",
    "Set **CUSTOM_CACHE_REPO** to the model repo you created for this training job, for instance: **user-name/model-name**. If you don't have a cache repo yet, just [follow the instructions in this page](https://huggingface.co/docs/optimum-neuron/guides/cache_system) and create one. Set **HF_TOKEN** to a valid Hugging Face access token generated in your account.\n",
    "\n",
    "If you don't set **HF_CACHE_REPO** and **HF_TOKEN** your model will be recompiled every time you invoke the training job and it consumes some time. It is **HIGHLY** recommended to use the cache mechanism to optimize this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c2359-567e-4d37-a5b4-577af3ec62a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Click on the \"model name\" in the table above to visualize which options of models you have to fine-tune\n",
    "# i.e: If you click on bert, bert-base-uncased is an available option to select\n",
    "MODEL=\"bert-base-uncased\"\n",
    "TASK=\"SequenceClassification\"\n",
    "HF_CACHE_REPO=\"\"\n",
    "HF_TOKEN=\"\"\n",
    "assert len(MODEL)>0, \"Please, use the table above to define a valid model name\"\n",
    "assert TASK in df_training.columns, \"Please, use the table above to define a valid task name\"\n",
    "assert len(HF_CACHE_REPO)>0, \"Please, set a valid Hugging Face CACHE REPO\"\n",
    "assert len(HF_TOKEN)>0, \"Please, set a valid HF access token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216e034-62a5-40cf-bc00-c63eabe3f0e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "if not os.path.isdir('src'): os.makedirs('src', exist_ok=True)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a27737-3dbd-46f0-b098-e6ff72627b49",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1) Training script that will be invoked by SageMaker\n",
    "\n",
    "This training script makes use of HF Optimum Neuron API to simplify the process. [You can learn more here](https://huggingface.co/docs/optimum-neuron/quickstart). This script is intented to show how to prepare a training job and quickly fine-tune a model. Depending on your needs you'll need to adjust/modify this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849bb42-1a85-489a-b621-615f9f1d917b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/train.py\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import evaluate\n",
    "import importlib\n",
    "import traceback\n",
    "import transformers\n",
    "\n",
    "from huggingface_hub import login\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, is_torch_tpu_available\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=500)\n",
    "    parser.add_argument(\"--tensor_parallel_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--model_id\", type=str, required=True)\n",
    "    parser.add_argument(\"--zero_1\", type=bool, default=False)\n",
    "    parser.add_argument(\"--task\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--collator\", type=str, default=\"DefaultDataCollator\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
    "    parser.add_argument(\"--bf16\", type=bool, default=True)\n",
    "\n",
    "    # hugging face hub\n",
    "    parser.add_argument(\"--hf_token\", type=str, default='')\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--n_neurons\", type=str, default=os.environ[\"SM_NUM_NEURONS\"])\n",
    "    parser.add_argument(\"--training_dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n",
    "    parser.add_argument(\"--eval_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_EVAL\", None))\n",
    "\n",
    "    parser.add_argument('--checkpoints-path', type=str, help=\"Path where we'll save the cache\", default='/opt/ml/checkpoints')\n",
    "    try:\n",
    "        args, _ = parser.parse_known_args()\n",
    "\n",
    "        cache_dir = os.path.join(args.checkpoints_path, args.model_id)\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "        if len(args.hf_token) > 0:\n",
    "            print(\"HF token defined. Logging in...\")\n",
    "            login(token=args.hf_token)\n",
    "\n",
    "        # Set up logging\n",
    "        logger = logging.getLogger(__name__)\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=logging.getLevelName(\"INFO\"),\n",
    "            handlers=[logging.StreamHandler(sys.stdout)],\n",
    "            format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "        )\n",
    "        os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "        os.environ['NEURON_CC_FLAGS']=f\"--cache_dir={cache_dir} --retry_failed_compilation\"\n",
    "\n",
    "        from optimum.neuron import NeuronTrainer as Trainer\n",
    "        from optimum.neuron import NeuronTrainingArguments as TrainingArguments\n",
    "\n",
    "        Collator = eval(f\"transformers.{args.collator}\")\n",
    "        AutoModel = eval(f\"transformers.AutoModel{'For' + args.task if len(args.task) > 0 else ''}\")\n",
    "\n",
    "        train_dataset=load_from_disk(args.training_dir)\n",
    "        eval_dataset=load_from_disk(args.eval_dir) if not args.eval_dir is None else None\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        data_collator = Collator(return_tensors=\"pt\")\n",
    "        model = AutoModel.from_pretrained(args.model_id, trust_remote_code=True) # TODO: add a hyperparameter with model params\n",
    "        model.config.output_attentions == True\n",
    "\n",
    "        def preprocess_logits_for_metrics(logits, labels):\n",
    "            if isinstance(logits, tuple):\n",
    "                # Depending on the model and config, logits may contain extra tensors,\n",
    "                # like past_key_values, but logits always come first\n",
    "                logits = logits[0]\n",
    "            return logits.argmax(dim=-1)\n",
    "\n",
    "        metric = evaluate.load(\"accuracy\", module_type=\"metric\")\n",
    "        def compute_metrics(eval_pred):\n",
    "            preds,labels = eval_pred\n",
    "            if len(preds.shape) == 1: preds = torch.IntTensor(preds).reshape(1,-1)\n",
    "            if len(labels.shape) == 1: labels = torch.IntTensor(labels).reshape(1,-1)    \n",
    "            for pred,label in zip(preds,labels):\n",
    "                metric.add_batch(predictions=pred, references=label)\n",
    "            return metric.compute()\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=args.learning_rate,\n",
    "            weight_decay=args.weight_decay,\n",
    "            bf16=args.bf16,\n",
    "            num_train_epochs=args.epochs,\n",
    "            output_dir=args.model_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            tensor_parallel_size=args.tensor_parallel_size,\n",
    "            zero_1=args.zero_1,\n",
    "\n",
    "            per_device_train_batch_size=args.train_batch_size,\n",
    "            per_device_eval_batch_size=args.eval_batch_size,\n",
    "            logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=500,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=2,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "        if not args.eval_dir is None:\n",
    "            eval_results = trainer.evaluate()\n",
    "\n",
    "            # writes eval result to file which can be accessed later in s3 ouput\n",
    "            with open(os.path.join(args.output_data_dir, \"eval_results.txt\"), \"w\") as writer:\n",
    "                print(f\"***** Eval results *****\")\n",
    "                for key, value in sorted(eval_results.items()):\n",
    "                    writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "        # Saves the model to s3\n",
    "        trainer.save_model()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        print(\"Done! \", sys.exc_info())\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a6f646-8306-4572-b4d1-496e8012bd20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "datasets\n",
    "evaluate\n",
    "accelerate\n",
    "torchvision\n",
    "scikit-learn\n",
    "neuronx-distributed\n",
    "## 4.30 or higher is required\n",
    "transformers==4.30.0\n",
    "optimum-neuron==0.0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f815449-7358-414a-ba98-bfa8afc69432",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2) Defining a SageMaker Estimator\n",
    "This object will help you to configure the training job and set the required hyperparameters + other config settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558c6bd9-aa4a-44b9-86ee-d4a646d17453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.trn1.2xlarge',\n",
    "    disable_profiler=True,\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.12.0-ubuntu20.04\",\n",
    "    \n",
    "    # Parameters required to enable checkpointing\n",
    "    # This is necessary for caching XLA HLO files and reduce training time next time    \n",
    "    checkpoint_s3_uri=f\"s3://{bucket}/checkpoints\",\n",
    "    volume_size = 512,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    },\n",
    "    environment={\n",
    "        \"XLA_USE_BF16\": \"1\",\n",
    "        \"OMP_NUM_THREADS\": \"2\",\n",
    "        \"FI_EFA_FORK_SAFE\": \"1\",        \n",
    "        \"NEURON_RT_STOCHASTIC_ROUNDING_EN\": \"1\",\n",
    "        \n",
    "        \"CUSTOM_CACHE_REPO\": HF_CACHE_REPO\n",
    "    },\n",
    "    hyperparameters={\n",
    "        \"model_id\": MODEL,\n",
    "        \"task\": TASK,        \n",
    "        \"bf16\": True,\n",
    "        \n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"epochs\": 25,\n",
    "        \"train_batch_size\": 4,\n",
    "        \"eval_batch_size\": 4,\n",
    "        \n",
    "        \"hf_token\": HF_TOKEN,\n",
    "        \n",
    "        #\"collator\": \"DataCollatorForLanguageModeling\",\n",
    "        #\"tensor_parallel_size\": 8,        \n",
    "    },\n",
    "    metric_definitions=[        \n",
    "        {\"Name\": \"eval_loss\", \"Regex\": \".eval_loss.:\\S*(.*?),\"},\n",
    "        {\"Name\": \"train_loss\", \"Regex\": \".train_loss.:\\S*(.*?),\"},\n",
    "        {\"Name\": \"it_per_sec\", \"Regex\": \",\\S*(.*?)it.s.\"},\n",
    "    ]\n",
    ")\n",
    "estimator.framework_version = '1.13.1' # workround when using image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ca2ac-4159-4e91-99d5-7dfd58d22a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_uri=f\"s3://{bucket}/datasets/spam/train\"\n",
    "eval_uri=f\"s3://{bucket}/datasets/spam/eval\"\n",
    "print(f\"{train_uri}\\n{eval_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307d504-7dbc-45e5-94d1-d50904d926b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.fit({\"train\": train_uri, \"eval\": eval_uri})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ede73-34ab-473f-88b4-f5585ac7b1f7",
   "metadata": {},
   "source": [
    "## 4) Now it is time to deploy our model\n",
    "\n",
    "[Open Deployment/Inference Notebook](03_ModelInference.ipynb)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-cpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
