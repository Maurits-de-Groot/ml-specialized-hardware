{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea34d80-1592-48b0-905b-845f29437577",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker==2.232.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9604ec03-8aeb-4c31-a688-62163172c277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "session_bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe07bddd-7ee0-40e1-af62-ac0c0cf530bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries for using Hugging Face models and SageMaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# Define the instance type that will be used for inference\n",
    "# ml.inf2.24xlarge is based on AWS Inferentia2 hardware, optimized for high-performance machine learning inference\n",
    "instance_type = \"ml.inf2.24xlarge\"\n",
    "\n",
    "# Set the health check timeout and volume size for the SageMaker model endpoint\n",
    "health_check_timeout = 2400  # The maximum time (in seconds) SageMaker waits for the model to be ready\n",
    "volume_size = 128  # Storage size in GB allocated to the model\n",
    "\n",
    "# Define the environment configuration for the Hugging Face model\n",
    "config = {\n",
    "    \"HF_MODEL_ID\": \"meta-llama/Meta-Llama-3.1-8B\",  # Hugging Face model ID\n",
    "    \"HF_NUM_CORES\": \"8\",  # Number of Neuron cores to use for inference\n",
    "    \"HF_AUTO_CAST_TYPE\": \"bf16\",  # Enable automatic casting to bf16 (half precision for faster inference)\n",
    "    \"MAX_BATCH_SIZE\": \"4\",  # Maximum batch size to process in one forward pass\n",
    "    \"MAX_INPUT_LENGTH\": \"4095\",  # Maximum input sequence length (tokens) allowed for inference\n",
    "    \"MAX_TOTAL_TOKENS\": \"4096\",  # Maximum total number of tokens (input + output)\n",
    "    \"HF_TOKEN\": \"<put your HF token there>\"  # Token to authenticate with Hugging Face Hub (ensure to keep this secure)\n",
    "}\n",
    "\n",
    "# Set the URI for the Hugging Face TGI (Text Generation Inference) image\n",
    "# This image is designed for optimized inference using AWS Neuron SDK (for Inferentia)\n",
    "tgi_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.2-optimum0.0.25-neuronx-py310-ubuntu22.04\"\n",
    "\n",
    "# Create the HuggingFaceModel object with the specified role, image, and environment configuration\n",
    "model = HuggingFaceModel(\n",
    "  role=role,  # IAM role that grants SageMaker permissions\n",
    "  image_uri=tgi_image,  # URI for the Hugging Face inference image\n",
    "  env=config  # Pass the environment variables defined in the config\n",
    ")\n",
    "\n",
    "# In this case, we are deploying a precompiled model, stored at https://huggingface.co/aws-neuron/optimum-neuron-cache \n",
    "# If the model you need to deploy the model that is not precompiled,  you can export your own neuron model\n",
    "# as explained in https://huggingface.co/docs/optimum-neuron/main/en/guides/export_model#exporting-neuron-models-using-neuronx-tgi\n",
    "\n",
    "# Mark the model as precompiled\n",
    "model._is_compiled_model = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62f5ed8d-77e6-45a8-8ba4-c35566631a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout,\n",
    "  volume_size=volume_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a8ddbde-4dc7-4b85-bd01-1311b78987b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What are the pros and cons of different energy sources? Is there a link between electricity usage and climate change? How can we tackle energy poverty, the issue of clean air at home, or the challenge of providing electricity access in refugee camps? Why should we care about these issues? And how can we better communicate these issues to diverse audiences?\\nThese are key issues for the energy sector – both at home and abroad. This degree will equip you to address them from the perspective of economics, innovation and policy – and prepare you for an exciting career.\\nOur innovative'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"inputs\": \"What are the pros and cons of different energy sources?\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 100,\n",
    "    \"top_p\": 0.9,\n",
    "    \"n\": 1,\n",
    "}\n",
    "\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38015e6b-82be-4e9a-bc1e-b52e95b21937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean-up\n",
    "\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2bb14b-a5a2-46c7-80c9-bbc4d301d20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
