{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1846be3f",
   "metadata": {},
   "source": [
    "# Feature extraction from text to measure document similarities\n",
    "\n",
    "Create a mechanism to extract features (embeddings) from text inputs. With the embeddings you can then compute the distance between two or more sentences. This is useful if you're building a search mechanism or trying to see how **\"semantically\"** two sentences are close.\n",
    "\n",
    "For that purpose you'll use a **[Bert base](https://huggingface.co/bert-base-cased-finetuned-mrpc)** model, accelerated by an inf1 instance ([AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)), running on SageMaker.\n",
    "\n",
    "For maximum performance and flexibility, you'll prepare the model with \"Neuron Core Pipeline\" and \"Dynamic Batch Size\" enabled. The first technique will shard the model across multiple cores to improve throughput. The second technique will allow you to send requests with different batch sizes. [Read more about these feature here](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/pipeline_tutorial/neuroncore_pipeline_pytorch.html).\n",
    "\n",
    "The text samples used in this notebook were extracted from: https://www.gutenberg.org/cache/epub/84/pg84-images.html#chap01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0219d4e5",
   "metadata": {},
   "source": [
    "## 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8011d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "# now restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d891e343",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U torch-neuron==1.10.1.2.2.0.0 neuron-cc[tensorflow] \"protobuf<4\" \"transformers==4.12.0\"\n",
    "# use --force-reinstall if you're facing some issues while loading the modules\n",
    "# now restart the kernel again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b645339",
   "metadata": {},
   "source": [
    "## 2) Initialize some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8912b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.neuron\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM']='false'\n",
    "\n",
    "max_length = 128\n",
    "save_dir = 'model'\n",
    "neuroncore_pipeline_cores=4\n",
    "model_id = 'bert-base-cased-finetuned-mrpc'\n",
    "# create dummy input for max length 128\n",
    "dummy_input_1 = \"I am by birth a Genevese, and my family is one of the most distinguished of that republic.\"\n",
    "dummy_input_2 = \"My ancestors had been for many years counsellors and syndics, and my father had filled several public situations with honour and reputation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36ce70",
   "metadata": {},
   "source": [
    "## 3) Load a tokenizer and compile the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id, return_dict = False, torchscript=True)\n",
    "\n",
    "encoded_input = tokenizer.encode_plus(\n",
    "                                dummy_input_1,\n",
    "                                dummy_input_2,\n",
    "                                max_length=max_length, \n",
    "                                padding=\"max_length\",\n",
    "                                return_tensors=\"pt\"\n",
    "                            )\n",
    "example_encoded_inputs = encoded_input['input_ids'], encoded_input['attention_mask'], encoded_input['token_type_ids']\n",
    "\n",
    "# save tokenizer, neuron model and config for later use\n",
    "if not os.path.isdir(save_dir): os.mkdir(save_dir)\n",
    "\n",
    "print(f'Loading a pre-trained model')\n",
    "model.eval()    \n",
    "y = model(**encoded_input) # warmup the model\n",
    "\n",
    "try:\n",
    "    traced_model = torch.jit.trace(model, example_encoded_inputs)\n",
    "    print(\"Cool! Model is jit traceable\")\n",
    "except Exception as e:\n",
    "    print(\"Ops. Something went wrong. Model is not traceable\")\n",
    "    \n",
    "\n",
    "# compile model with torch.neuron.trace and update config\n",
    "model_neuron = torch.neuron.trace(\n",
    "    model, example_encoded_inputs, \n",
    "    dynamic_batch_size=True,\n",
    "    compiler_args = ['--neuroncore-pipeline-cores', str(neuroncore_pipeline_cores)]\n",
    ")\n",
    "model.config.update({\"traced_sequence_length\": max_length})\n",
    "\n",
    "model_neuron.save(os.path.join(save_dir,\"model_neuron.pt\"))\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "model.config.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6242872c",
   "metadata": {},
   "source": [
    "### 3.1) Upload the compiled model to an S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58abb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import tarfile\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = sess.default_bucket()\n",
    "with io.BytesIO() as file:\n",
    "    with tarfile.open(fileobj=file, mode=\"w:gz\") as tar:\n",
    "        tar.add(save_dir, \".\")\n",
    "        tar.list()\n",
    "    file.seek(0)\n",
    "    s3_uri = sess.upload_string_as_file_body(\n",
    "        file.read(), sagemaker_session_bucket, \"model/bert/model.tar.gz\"\n",
    "    )\n",
    "print(s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594294cb",
   "metadata": {},
   "source": [
    "## 4) Inference script used by SageMaker endpoint to load and execute the model\n",
    "This script is responsible for loading the model and expose a webservice for us to invoke and get predictions (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34d49e",
   "metadata": {},
   "source": [
    "## 5) Deploy our model to a SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34144be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e576d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_uri,       # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.12\",  # transformers version used\n",
    "   pytorch_version=\"1.9\",        # pytorch version used\n",
    "   py_version='py37',            # python version used\n",
    "   sagemaker_session=sess,\n",
    "   model_server_workers=4, # keep 4 workers\n",
    "   entry_point=\"code/inference.py\",\n",
    "   # for production it is important to define vpc_config and use a vpc_endpoint\n",
    "   #vpc_config={\n",
    "   #    'Subnets': ['subnet-a320a8ca', 'subnet-56d5072d'],\n",
    "   #    'SecurityGroupIds': ['sg-0d8c231d83c1caaa6', 'sg-5504723c']\n",
    "   #}    \n",
    ")\n",
    "\n",
    "# Let SageMaker know that we've already compiled the model via neuron-cc\n",
    "huggingface_model._is_compiled_model = True\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,      # number of instances\n",
    "    instance_type=\"ml.inf1.6xlarge\" # AWS Inferentia Instance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7696086b",
   "metadata": {},
   "source": [
    "## 6) Run a simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a165eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import NumpyDeserializer\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = NumpyDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0778ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('frank_chap01.txt') as f:\n",
    "    data = {'inputs': [l.strip() for l in f.readlines()]}\n",
    "num_sentences = len(data['inputs'])\n",
    "print(f\"Number of sentences: {num_sentences}\")\n",
    "embeddings = predictor.predict(data)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b62634",
   "metadata": {},
   "source": [
    "### 6.1) Simple benchmark to identify the best batch_size with 1 client only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "946b3d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1 Elapsed time: 14.544463157653809ms Latency p/s 14.544463157653809ms\n",
      "Batch size: 2 Elapsed time: 23.25267791748047ms Latency p/s 11.626338958740234ms\n",
      "Batch size: 3 Elapsed time: 31.86509609222412ms Latency p/s 10.621698697408041ms\n",
      "Batch size: 4 Elapsed time: 39.96927738189697ms Latency p/s 9.992319345474243ms\n",
      "Batch size: 5 Elapsed time: 48.52888584136963ms Latency p/s 9.705777168273926ms\n",
      "Batch size: 6 Elapsed time: 57.08444118499756ms Latency p/s 9.514073530832926ms\n",
      "Batch size: 7 Elapsed time: 65.29092788696289ms Latency p/s 9.32727541242327ms\n",
      "Batch size: 8 Elapsed time: 74.49376583099365ms Latency p/s 9.311720728874207ms\n",
      "Batch size: 9 Elapsed time: 82.37555027008057ms Latency p/s 9.15283891889784ms\n",
      "Batch size: 10 Elapsed time: 90.54069519042969ms Latency p/s 9.054069519042969ms\n",
      "Batch size: 11 Elapsed time: 99.27759170532227ms Latency p/s 9.025235609574752ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "iterations=10\n",
    "for batch_size in range(1,num_sentences+1):\n",
    "    d = copy.deepcopy(data)\n",
    "    d['inputs'] = d['inputs'][:batch_size]\n",
    "    t=time.time()\n",
    "    for i in range(iterations):\n",
    "        predictor.predict(d)\n",
    "    elapsed = (time.time()-t)/iterations*1000\n",
    "    print(f\"Batch size: {batch_size} Elapsed time: {elapsed}ms Latency p/s {elapsed/batch_size}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19be8b4",
   "metadata": {},
   "source": [
    "### 6.2) Now Invoke the endpoint in parallel to evaluate throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "291ffce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 24082.525491714478ms to process 11264 sentences with 5 workers. Latency p/s: 2.1380083000456747ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# custom task that will sleep for a variable amount of time\n",
    "def task(data):\n",
    "    predictor.predict(data)\n",
    "\n",
    "num_workers = 5\n",
    "d = copy.deepcopy(data)\n",
    "documents_1k = [d for i in range(1024)]\n",
    "total_docs = len(documents_1k) * len(data['inputs'])\n",
    "\n",
    "# start the thread pool\n",
    "t=time.time()\n",
    "with ThreadPoolExecutor(num_workers) as executor:\n",
    "    # execute tasks concurrently and process results in order    \n",
    "    executor.map(task, documents_1k)\n",
    "elapsed = (time.time()-t)*1000\n",
    "print(f\"Elapsed time: {elapsed}ms to process {total_docs} sentences with {num_workers} workers. Latency p/s: {elapsed/total_docs}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad860fd",
   "metadata": {},
   "source": [
    "### 6.3) Finally a similarity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2a901ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: [[0.9238203]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sentence_1=\"I've seen things you people wouldn't believe. Attack ships on fire off the shoulder of Orion.\"\n",
    "sentence_2=\"I watched C-beams glitter in the dark near the Tannhäuser Gate. All those moments will be lost in time, like tears in rain. Time to die.\"\n",
    "embeddings_1,embeddings_2 = predictor.predict({'inputs':[sentence_1, sentence_2]})\n",
    "print(f'Cosine Similarity: {cosine_similarity([embeddings_1],[embeddings_2])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
