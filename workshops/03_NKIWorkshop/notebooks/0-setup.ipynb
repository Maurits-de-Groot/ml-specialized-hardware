{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - setup\n",
    "In this guide, we will implement a simple “Hello World” style NKI kernel and run it on a NeuronDevice (Trainium/Inferentia2 or beyond device). We will showcase how to invoke a NKI kernel standalone through NKI baremetal mode and also through ML frameworks (PyTorch). Before diving into kernel implementation, let’s make sure you have the correct environment setup for running NKI kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "You need a [Trn1](https://aws.amazon.com/ec2/instance-types/trn1/) or [Inf2](https://aws.amazon.com/ec2/instance-types/inf2/) instance set up on AWS to run NKI kernels on a NeuronDevice. Once logged into the instance, follow steps below to ensure you have all the required packages installed in your Python environment.\n",
    "\n",
    "NKI is shipped as part of the Neuron compiler package. To make sure you have the latest compiler package, see [Setup Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/index.html) for an installation guide.\n",
    "\n",
    "You can verify that NKI is available in your compiler installation by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuronxcc.nki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This attempts to import the NKI package. It will error out if NKI is not included in your Neuron compiler version or if the Neuron compiler is not installed. The import might take about a minute the first time you run it. Whenever possible, we recommend using local instance NVMe volumes instead of EBS for executable code.\n",
    "\n",
    "If you intend to run NKI kernels without any ML framework for quick prototyping, you will also need NumPy installed.\n",
    "\n",
    "To call NKI kernels from PyTorch, you also need to have torch_neuronx installed. For an installation guide, see PyTorch Neuron Setup. You can verify that you have torch_neuronx installed by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_neuronx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing your first NKI kernel\n",
    "In current NKI release, all input and output tensors must be passed into the kernel as device memory (HBM) tensors on a NeuronDevice. The body of the kernel typically consists of three main phases:\n",
    "\n",
    "1. Load the inputs from device memory to on-chip memory (SBUF).\n",
    "2. Perform the desired computation.\n",
    "3. Store the outputs from on-chip memory to device memory.\n",
    "\n",
    "For more details on the above terms, see [NKI Programming Model](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuronxcc.nki.language as nl\n",
    "\n",
    "def nki_tensor_add_kernel(a_input, b_input, c_output):\n",
    "    \"\"\"\n",
    "    NKI kernel to compute element-wise addition of two input tensors\n",
    "    \"\"\"\n",
    "\n",
    "    # Check all input/output tensor shapes are the same for element-wise operation\n",
    "    assert a_input.shape == b_input.shape == c_output.shape\n",
    "\n",
    "    # Check size of the first dimension does not exceed on-chip memory tile size limit,\n",
    "    # so that we don't need to tile the input to keep this example simple\n",
    "    assert a_input.shape[0] <= nl.tile_size.pmax\n",
    "\n",
    "    # Load the inputs from device memory to on-chip memory\n",
    "    a_tile = nl.load(a_input)\n",
    "    b_tile = nl.load(b_input)\n",
    "\n",
    "    # Specify the computation (in our case: a + b)\n",
    "    c_tile = nl.add(a_tile, b_tile)\n",
    "\n",
    "    # Store the result to c_output from on-chip memory to device memory\n",
    "    nl.store(c_output, value=c_tile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NKI baremetal\n",
    "\n",
    "To run the above `nki_tensor_add_kernel` kernel in baremetal mode, we can decorate the function with `@baremetal` as follows:\n",
    "\n",
    "\n",
    "```python\n",
    "@baremetal\n",
    "def nki_tensor_add_kernel(a_input, b_input, c_output):\n",
    "```\n",
    "\n",
    "See [nki.baremetal](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.baremetal.html) API doc for available input arguments to the decorator. `nki.baremetal` expects input and output tensors of the NKI kernel to be NumPy arrays. To invoke the kernel, we first initialize the two input tensors `a` and `b` and the output tensor `c` as NumPy arrays. In this scenario, it’s not necessary to zero out the output tensor, as it will be completely overwritten by the result of the addition. However, in some cases, a kernel might overwrite only a part of the output tensor, and the user might want to reset it beforehand to avoid garbage data. Finally, we call the NKI kernel just like any other Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuronxcc.nki import baremetal\n",
    "\n",
    "\"\"\"\n",
    "Note that this is the same as: \n",
    "\n",
    "@baremetal\n",
    "def nki_tensor_add_kernel(a_input, b_input, c_output):\n",
    "\"\"\"\n",
    "nki_tensor_add_kernel_baremetal = baremetal(nki_tensor_add_kernel) \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "a = np.ones((4, 3), dtype=np.float16)\n",
    "b = np.ones((4, 3), dtype=np.float16)\n",
    "c = np.zeros((4, 3), dtype=np.float16)\n",
    "\n",
    "# Run NKI kernel on a NeuronDevice\n",
    "nki_tensor_add_kernel_baremetal(a, b, c)\n",
    "\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "To run the above `nki_tensor_add_kernel` kernel using PyTorch, we can decorate the function with `@nki_jit` as follows:\n",
    "\n",
    "```python\n",
    "@nki_jit\n",
    "def nki_tensor_add_kernel(a_input, b_input, c_output):\n",
    "```\n",
    "\n",
    "The kernel caller code is highly similar to NKI baremetal mode, except the input and output tensors must now be initialized as PyTorch `device` tensors instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_xla.core import xla_model as xm\n",
    "from torch_neuronx import nki_jit\n",
    "\n",
    "\"\"\"\n",
    "Note that this is the same as: \n",
    "\n",
    "@nki_jit\n",
    "def nki_tensor_add_kernel(a_input, b_input, c_output):\n",
    "\"\"\"\n",
    "nki_tensor_add_kernel_pytorch = nki_jit(nki_tensor_add_kernel)\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "a = torch.ones((4, 3), dtype=torch.float16).to(device=device)\n",
    "b = torch.ones((4, 3), dtype=torch.float16).to(device=device)\n",
    "c = torch.zeros((4, 3), dtype=torch.float16).to(device=device)\n",
    "\n",
    "nki_tensor_add_kernel_pytorch(a, b, c)\n",
    "\n",
    "print(c) # an implicit XLA barrier/mark-step (triggers XLA compilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Release the NeuronCore for the next notebook\n",
    "\n",
    "Before moving to the next notebook we need to release the NeuronCore. If we don't do this the next notebook will not be able resources - you can also stop the kernel via the GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
