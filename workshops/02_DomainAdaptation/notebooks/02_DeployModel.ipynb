{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ddcbb32-4762-4d1f-8932-43367100f7e9",
   "metadata": {},
   "source": [
    "# LLM Domain Adaptation with ORPO, AWS Trainium and AWS Inferentia2\n",
    "\n",
    "Language models are incredibly powerful, but adapting them to specific tasks can be challenging. Traditional approaches involve two separate stages: first, supervised fine-tuning to align the model with the desired domain, and then a preference alignment step to increase the likelihood of desirable outputs and reduce undesirable ones.\n",
    "\n",
    "However, this two-stage process has limitations. While supervised fine-tuning is effective at domain adaptation, it can inadvertently increase the chances of generating both preferred and undesired responses.\n",
    "\n",
    "To address this issue, techniques like reinforcement learning with human feedback (RLHF) or direct preference optimization (DPO) are often employed for preference alignment. These methods aim to sculpt the model's outputs towards desired responses and away from rejected ones. However, they require a separate reference model, adding computational complexity.\n",
    "\n",
    "Odds-Ratio Predictive Ordering (ORPO) offers an elegant solution by combining supervised fine-tuning and preference alignment into a single objective function. It modifies the standard language modeling loss by incorporating an odds ratio term that weakly penalizes rejected responses while strongly rewarding preferred ones.\n",
    "\n",
    "In essence, ORPO streamlines the adaptation process by simultaneously fine-tuning the model to the target domain and aligning its preferences towards desired outputs â€“ all within a single training objective. This unified approach simplifies the workflow and reduces computational overhead compared to traditional multi-stage methods.\n",
    "\n",
    "----\n",
    "This is the first notebook out of two parts. In this notebook you run a SageMaker job to compile your aligned mode (in the previous notebook) to AWS Inferentia2.\n",
    "Then you deploy the compiled model to a SageMaker endpoint and run some tests.\n",
    "\n",
    "**SageMaker Studio**: Jupyter Lab  \n",
    "**Kernel**: Python3  \n",
    "\n",
    "This exercise is divide into 2 parts:\n",
    " - Data prep + model alignment\n",
    " - **Model deployment + tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf8bbd-0322-41c4-a5da-d3d6add61809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "inf_region=\"us-east-2\"\n",
    "inf_sess = sagemaker.Session(boto_session=boto3.Session(region_name=inf_region))\n",
    "inf_bucket = inf_sess.default_bucket()\n",
    "\n",
    "train_region=inf_region\n",
    "train_sess=inf_sess\n",
    "train_bucket=inf_bucket\n",
    "\n",
    "train_job_name=\"\"\n",
    "if os.path.isfile(\"training_job_name.txt\"): \n",
    "    lines = open(\"training_job_name.txt\", \"r\").readlines()\n",
    "    train_job_name = lines[0].strip()\n",
    "    train_region = lines[1].strip()\n",
    "\n",
    "    train_sess = sagemaker.Session(boto_session=boto3.Session(region_name=train_region))\n",
    "    train_bucket = train_sess.default_bucket()\n",
    "assert len(train_job_name)>0, \"Please copy the name of the training_job you ran in the previous notebook and set training_job_name\"\n",
    "\n",
    "HF_TOKEN=\"\"\n",
    "tok_file = os.path.join(os.environ['HOME'], '.hf_token')\n",
    "if os.path.isfile(tok_file): HF_TOKEN=open(tok_file, 'r').read().strip()    \n",
    "assert HF_TOKEN != \"\", \" >>> Go to your HF account and get an access token. Set HF_TOKEN to your token if you want to define your own cache repo\"\n",
    "\n",
    "data_key=f\"output/{train_job_name}/output/model\"\n",
    "checkpoint_s3_uri=f\"s3://{train_bucket}/{data_key}\"\n",
    "\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket - train: {train_bucket}, inference: {inf_bucket}\")\n",
    "print(f\"sagemaker session region - train {train_region}, inference: {inf_region}\")\n",
    "print(f\"Training job name: {train_job_name}\")\n",
    "print(f\"Model S3 URI: {checkpoint_s3_uri}\")\n",
    "print(f\"HF Token found? {HF_TOKEN != ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c93ce-4080-43fd-b907-c381217d1bfb",
   "metadata": {},
   "source": [
    "## 1) Create compile/deploy artifacts\n",
    "### 1.1) Dependencies descriptor\n",
    "Installing the libraries listed in this file will be the first thing SageMaker will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5e3c1-8ec0-4727-af90-901c220eca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "--extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "optimum-neuron==0.0.26\n",
    "trl==0.11.4\n",
    "peft==0.13.2\n",
    "sentencepiece\n",
    "neuronx-cc==2.15.128.0+56dc5a86\n",
    "transformers-neuronx==0.12.313"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d420b-3163-420c-ab43-8664df8a5c24",
   "metadata": {},
   "source": [
    "### 1.2) Compile and deployment script\n",
    "The code executed inside __main__ will be used to compile the model. However, the same script will then be used to deploy a SageMaker endpoint later.\n",
    "For the model deployment, only the methods defined before __main__ will be used by SageMaker, for instance: **model_fn**, **predict_fn**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f537b40-b082-4dad-8944-8d20db62b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/compile.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import tarfile\n",
    "import logging\n",
    "import argparse\n",
    "import traceback\n",
    "from peft import PeftModel\n",
    "from trl import setup_chat_format\n",
    "from huggingface_hub import login\n",
    "from optimum.neuron import NeuronModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Defines a function model_fn that loads a tokenizer and a model from the specified directory.\n",
    "def model_fn(model_dir, context=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = NeuronModelForCausalLM.from_pretrained(model_dir)\n",
    "    return model,tokenizer\n",
    "\n",
    "# Defines an input_fn function to process incoming requests.\n",
    "def input_fn(input_data, content_type, context=None):\n",
    "    if content_type == 'application/json':\n",
    "        req = json.loads(input_data)\n",
    "        prompt = req.get('prompt')\n",
    "        temperature = req.get('temperature', 0.8)\n",
    "        top_p = req.get('top_p', 0.9)\n",
    "        if prompt is None or len(prompt) < 3:\n",
    "            raise(\"Invalid prompt. Provide an input like: {'prompt': 'text text text'}\")\n",
    "        return prompt,temperature,top_p\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported mime type: {content_type}. Supported: application/json\")    \n",
    "\n",
    "# Defines a predict_fn function that generates predictions based on user input.\n",
    "def predict_fn(input_object, model_tokenizer, context=None):\n",
    "    model,tokenizer = model_tokenizer\n",
    "    prompt,temperature,top_p = input_object\n",
    "\n",
    "    messages = [{'content': prompt, 'role': 'user'}, {'content': '', 'role': 'assistant'}]\n",
    "\n",
    "    # Test on sample\n",
    "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=int(os.environ.get(\"MAX_SEQ_LEN\", 512)),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    \n",
    "    response = outputs[0][input_ids.shape[-1]:] # remove input from output\n",
    "    response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    return {\"response\": response}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"Number of samples processed in each batch during training or inference\")\n",
    "    parser.add_argument(\"--tp_degree\", type=int, default=2, help=\"Degree of tensor parallelism to be used\")\n",
    "    parser.add_argument(\"--max_seq_len\", type=int, default=512, help=\"Maximum sequence length for input data\")\n",
    "    parser.add_argument(\"--max_prompt_len\", type=int, default=256, help=\"Specifying the maximum length of prompt\")\n",
    "    parser.add_argument(\"--hf_token\", type=str, default=None, help=\"Which is used for authentication with Hugging Face's model hub\")\n",
    "    parser.add_argument(\"--model_id\", type=str, default=\"meta-llama/Llama-3.2-1B\", help=\"Specifies the id for the pre-trained model to be used\")\n",
    "\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])    \n",
    "    parser.add_argument(\"--checkpoint_dir\", type=str, default=os.environ[\"SM_CHANNEL_CHECKPOINT\"])\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    if not args.hf_token is None and len(args.hf_token) > 0:\n",
    "        print(\"HF token defined. Logging in...\")\n",
    "        login(token=args.hf_token)\n",
    "    \n",
    "    compiler_args = {\"num_cores\": args.tp_degree, \"auto_cast_type\": 'bf16'}\n",
    "    input_shapes = {\"batch_size\": args.batch_size, \"sequence_length\": args.max_seq_len, \"prompt_len\": args.max_prompt_len}\n",
    "\n",
    "    #initializes a tokenizer using the AutoTokenizer class from the Hugging Face Transformers library.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_id).bfloat16()\n",
    "    model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,  # The base model to be used for prompt tuning\n",
    "        args.checkpoint_dir,   # The path where the trained Peft model is saved\n",
    "        is_trainable=False  # Indicates that the loaded model should not be trainable\n",
    "    )\n",
    "    model = model.merge_and_unload()\n",
    "    model.save_pretrained(\"merged_model\")\n",
    "    neuron_model = NeuronModelForCausalLM.from_pretrained(\"merged_model\", export=True,\n",
    "        **compiler_args,\n",
    "        **input_shapes)\n",
    "    neuron_model.save_pretrained(args.model_dir)\n",
    "    tokenizer.save_pretrained(args.model_dir)\n",
    "    \n",
    "    code_path = os.path.join(args.model_dir, 'code')\n",
    "    os.makedirs(code_path, exist_ok=True)\n",
    "\n",
    "    shutil.copy(__file__, os.path.join(code_path, \"inference.py\"))\n",
    "    shutil.copy('requirements.txt', os.path.join(code_path, 'requirements.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea349a1-cd52-4f56-b98b-ef2f91c63ba5",
   "metadata": {},
   "source": [
    "## 2) Kick-off the compiling job\n",
    "First we create a [SageMaker Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) with all the parameters we need to launch a compiling job.\n",
    "\n",
    "It takes ~9 mins to compile a Llama3.2-1B model using 1 trn1.2xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc164e-250f-437e-be2a-5934859a0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "tp_degree=2\n",
    "max_seq_len=512\n",
    "\n",
    "hyperparameters={\n",
    "    \"max_seq_len\": max_seq_len,\n",
    "    \"max_prompt_len\": 256,\n",
    "    \"tp_degree\": tp_degree,\n",
    "    \"batch_size\": 1,\n",
    "    \"model_id\": \"meta-llama/Llama-3.2-1B\"\n",
    "}\n",
    "\n",
    "if HF_TOKEN and len(HF_TOKEN) > 3:\n",
    "    hyperparameters[\"hf_token\"]= HF_TOKEN\n",
    "    \n",
    "estimator = PyTorch(\n",
    "    entry_point=\"compile.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=train_sess,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.trn1.2xlarge',\n",
    "    output_path=f\"s3://{train_bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    disable_output_compression=True,\n",
    "    \n",
    "    image_uri=f\"763104351884.dkr.ecr.{train_region}.amazonaws.com/pytorch-training-neuronx:2.1.2-neuronx-py310-sdk2.20.0-ubuntu20.04\",\n",
    "    env={\n",
    "        'NEURON_RT_NUM_CORES': str(tp_degree)\n",
    "    },\n",
    "    volume_size = 512,\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb655a-8d7e-4f9a-bf58-60e8431619d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## This will take ~9mins\n",
    "estimator.fit({\"checkpoint\": checkpoint_s3_uri})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16951708-093a-4d83-b176-70301d31b6a7",
   "metadata": {},
   "source": [
    "## 3) Deploy the compiled model to a SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222fd74b-0bfb-4e6e-a937-c163e12c4435",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_region != inf_region:\n",
    "    print(f\"We trained our model in region {train_region} but will deploy to {inf_region}\")\n",
    "    # We need to move the weights from the training region to the deployment region\n",
    "    \n",
    "    source_s3_uri=estimator.model_data['S3DataSource']['S3Uri']\n",
    "    target_s3_uri=f\"s3://{inf_bucket}/{source_s3_uri.split('/', 3)[-1]}\"\n",
    "    print(f\"Copying data from {source_s3_uri} to {target_s3_uri}\")\n",
    "    !aws s3 sync $source_s3_uri $target_s3_uri\n",
    "    model_data = {'S3DataSource': {'S3Uri': target_s3_uri, 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}\n",
    "else:\n",
    "    model_data = estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46f381-7fdc-4907-b2e3-d0d87030b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "\n",
    "print(f\"Model data: {model_data}\")\n",
    "\n",
    "instance_type=\"ml.inf2.xlarge\"\n",
    "num_workers=1\n",
    "\n",
    "print(f\"Instance type: {instance_type}. Num SM workers: {num_workers}\")\n",
    "pytorch_model = PyTorchModel(\n",
    "    image_uri=f\"763104351884.dkr.ecr.{inf_region}.amazonaws.com/pytorch-inference-neuronx:2.1.2-neuronx-py310-sdk2.20.0-ubuntu20.04\",\n",
    "    model_data=model_data,\n",
    "    role=role,    \n",
    "    name=name_from_base('orpo-llama3'),\n",
    "    sagemaker_session=inf_sess,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    model_server_workers=num_workers,\n",
    "    framework_version=\"2.1.2\",\n",
    "    env = {\n",
    "        'SAGEMAKER_MODEL_SERVER_TIMEOUT': '3600',\n",
    "        'MAX_SEQ_LEN': str(max_seq_len),\n",
    "        'NEURON_RT_NUM_CORES': str(tp_degree)\n",
    "    }\n",
    "    # for production it is important to define vpc_config and use a vpc_endpoint\n",
    "    #vpc_config={\n",
    "    #    'Subnets': ['<SUBNET1>', '<SUBNET2>'],\n",
    "    #    'SecurityGroupIds': ['<SECURITYGROUP1>', '<DEFAULTSECURITYGROUP>']\n",
    "    #}\n",
    ")\n",
    "pytorch_model._is_compiled_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57008b-1140-4c11-8750-055ad1fd6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    model_data_download_timeout=3600, # it takes some time to download all the artifacts and load the model\n",
    "    container_startup_health_check_timeout=1800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a9da6-dbea-4217-9c04-bbe9e5b42171",
   "metadata": {},
   "source": [
    "## 4) Run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777315c4-b4fe-4d7f-837d-f972076fb803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f97ca-1f62-48a4-a2d6-e24408c15fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#prompt = \"Explain the traditional techniques involved in cultivating a Bonsai tree.\"\n",
    "prompt = \"June and Julia live 1 mile apart. It takes June 4 minutes to ride her bike directly to Julia's house. At the same rate, how many minutes would it take June to ride the 3.5 miles from her own house to Bernard's house?\"\n",
    "#prompt = \"What is necessary to have a rainbow?\"\n",
    "t=time.time()\n",
    "pred = predictor.predict({\"prompt\": prompt})\n",
    "elapsed = (time.time()-t)*1000\n",
    "print(f\"Elapsed time: {elapsed}ms\")\n",
    "print(f\"Pred: {pred['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64488f2e-a6a8-49c5-8e55-8a0e8465a8d6",
   "metadata": {},
   "source": [
    "## 5) Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d40fa-5fd0-4488-8e61-6458d667336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906fe91-5aec-42cc-a3fa-e625cf7456ea",
   "metadata": {},
   "source": [
    "Done! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05da9da-c9cb-4aa1-97b4-ab09a3779bb9",
   "metadata": {},
   "source": [
    "## 6) Bonus\n",
    "\n",
    "If you have additional time, go back to notebook 1 and change the dataset by filtering the 25+ sources and selecting only the ones you want to design a new agent.  \n",
    "For instance: If you need an agent that does **reasoning process** keep **distillabel-orca**; if you want **instruction-following**, keep **ultrafeedback** and so one. \n",
    "\n",
    "Use the information printed by the first cells in notebook 1. Look for **Mixed datasets** in the output to check the list of all mixed source datasets you can play with.\n",
    "\n",
    "Than, repeat re-execute both notebooks to see the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
