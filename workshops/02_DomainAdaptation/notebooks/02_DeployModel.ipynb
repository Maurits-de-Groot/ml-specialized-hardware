{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ddcbb32-4762-4d1f-8932-43367100f7e9",
   "metadata": {},
   "source": [
    "# LLM Domain Adaptation with ORPO, AWS Trainium and AWS Inferentia2\n",
    "\n",
    "**SageMaker Studio**: Jupyter Lab  \n",
    "**Kernel**: Python3  \n",
    "\n",
    "This exercise is divide into 2 parts:\n",
    " - Data prep + model alignment\n",
    " - **Model deployment + tests**\n",
    "\n",
    "In this notebook you'll run the second part. You start by running a SageMaker job to compile your aligned mode (in the previous notebook) to AWS Inferentia2.\n",
    "Then you deploy a SageMaker endpoint with the compiled model and run some tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096bad31-3e81-467d-946a-b3450c9d587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf8bbd-0322-41c4-a5da-d3d6add61809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "training_job_name=\"\"\n",
    "HF_TOKEN=\"\"\n",
    "if os.path.isfile(\"training_job_name.txt\"): \n",
    "    lines = open(\"training_job_name.txt\", \"r\").readlines()\n",
    "    training_job_name = lines[0].strip()\n",
    "    HF_TOKEN = lines[1].strip()\n",
    "assert len(training_job_name)>0, \"Please copy the name of the training_job you ran in the previous notebook and set training_job_name\"\n",
    "\n",
    "checkpoint_s3_uri=f\"s3://{bucket}/output/{training_job_name}/output/model\"\n",
    "\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")\n",
    "print(f\"Training job name: {training_job_name}\")\n",
    "print(f\"Model S3 URI: {checkpoint_s3_uri}\")\n",
    "print(f\"HF Token found? {HF_TOKEN != ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c93ce-4080-43fd-b907-c381217d1bfb",
   "metadata": {},
   "source": [
    "## 1) Create compile/deploy artifacts\n",
    "### 1.1) Dependencies descriptor\n",
    "Installing the libraries listed in this file will be the first thing SageMaker will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5e3c1-8ec0-4727-af90-901c220eca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "--extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "git+https://github.com/huggingface/optimum-neuron@02c331d\n",
    "trl==0.11.4\n",
    "peft==0.13.2\n",
    "neuronx-cc==2.15.128.0+56dc5a86\n",
    "transformers-neuronx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d420b-3163-420c-ab43-8664df8a5c24",
   "metadata": {},
   "source": [
    "### 1.2) Compile and deployment script\n",
    "The code executed inside __main__ will be used to compile the model. However, the same script will then be used to deploy a SageMaker endpoint later.\n",
    "For the model deployment, only the methods defined before __main__ will be used by SageMaker, for instance: **model_fn**, **predict_fn**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f537b40-b082-4dad-8944-8d20db62b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/compile.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import tarfile\n",
    "import logging\n",
    "import argparse\n",
    "import traceback\n",
    "from peft import PeftModel\n",
    "from trl import setup_chat_format\n",
    "from huggingface_hub import login\n",
    "from optimum.neuron import NeuronModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def model_fn(model_dir, context=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = NeuronModelForCausalLM.from_pretrained(model_dir)\n",
    "    return model,tokenizer\n",
    "\n",
    "def input_fn(input_data, content_type, context=None):\n",
    "    if content_type == 'application/json':\n",
    "        req = json.loads(input_data)\n",
    "        prompt = req.get('prompt')\n",
    "        temperature = req.get('temperature', 0.8)\n",
    "        top_p = req.get('top_p', 0.9)\n",
    "        if prompt is None or len(prompt) < 3:\n",
    "            raise(\"Invalid prompt. Provide an input like: {'prompt': 'text text text'}\")\n",
    "        return prompt,temperature,top_p\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported mime type: {content_type}. Supported: application/json\")    \n",
    "\n",
    "def predict_fn(input_object, model_tokenizer, context=None):\n",
    "    model,tokenizer = model_tokenizer\n",
    "    prompt,temperature,top_p = input_object\n",
    "\n",
    "    messages = [{'content': prompt, 'role': 'user'}, {'content': '', 'role': 'assistant'}]\n",
    "\n",
    "    # Test on sample\n",
    "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=int(os.environ.get(\"MAX_SEQ_LEN\", 512)),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    \n",
    "    response = outputs[0][input_ids.shape[-1]:] # remove input from output\n",
    "    response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    return {\"response\": response}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--tp_degree\", type=int, default=2)\n",
    "    parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
    "    parser.add_argument(\"--max_prompt_len\", type=int, default=256)\n",
    "    parser.add_argument(\"--hf_token\", type=str, default=None)\n",
    "    parser.add_argument(\"--model_id\", type=str, default=\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])    \n",
    "    parser.add_argument(\"--checkpoint_dir\", type=str, default=os.environ[\"SM_CHANNEL_CHECKPOINT\"])\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    if not args.hf_token is None and len(args.hf_token) > 0:\n",
    "        print(\"HF token defined. Logging in...\")\n",
    "        login(token=args.hf_token)\n",
    "    \n",
    "    compiler_args = {\"num_cores\": args.tp_degree, \"auto_cast_type\": 'bf16'}\n",
    "    input_shapes = {\"batch_size\": args.batch_size, \"sequence_length\": args.max_seq_len, \"prompt_len\": args.max_prompt_len}\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_id).bfloat16()\n",
    "    model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,  # The base model to be used for prompt tuning\n",
    "        args.checkpoint_dir,   # The path where the trained Peft model is saved\n",
    "        is_trainable=False  # Indicates that the loaded model should not be trainable\n",
    "    )\n",
    "    model = model.merge_and_unload()\n",
    "    model.save_pretrained(\"merged_model\")\n",
    "    neuron_model = NeuronModelForCausalLM.from_pretrained(\"merged_model\", export=True,\n",
    "        **compiler_args,\n",
    "        **input_shapes)\n",
    "    neuron_model.save_pretrained(args.model_dir)\n",
    "    tokenizer.save_pretrained(args.model_dir)\n",
    "    \n",
    "    code_path = os.path.join(args.model_dir, 'code')\n",
    "    os.makedirs(code_path, exist_ok=True)\n",
    "\n",
    "    shutil.copy(__file__, os.path.join(code_path, \"inference.py\"))\n",
    "    shutil.copy('requirements.txt', os.path.join(code_path, 'requirements.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea349a1-cd52-4f56-b98b-ef2f91c63ba5",
   "metadata": {},
   "source": [
    "## 2) Kick-off the compiling job\n",
    "First we create a [SageMaker Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) with all the parameters we need to launch a compiling job.\n",
    "\n",
    "It takes ~9 mins to compile a Llama3.2-1B model using 1 trn1.2xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc164e-250f-437e-be2a-5934859a0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "tp_degree=2\n",
    "max_seq_len=512\n",
    "\n",
    "hyperparameters={\n",
    "    \"max_seq_len\": max_seq_len,\n",
    "    \"max_prompt_len\": 256,\n",
    "    \"tp_degree\": tp_degree,\n",
    "    \"batch_size\": 1,\n",
    "    \"model_id\": \"meta-llama/Llama-3.2-1B\"\n",
    "}\n",
    "\n",
    "if HF_TOKEN and len(HF_TOKEN) > 3:\n",
    "    hyperparameters[\"hf_token\"]= HF_TOKEN\n",
    "    \n",
    "estimator = PyTorch(\n",
    "    entry_point=\"compile.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.trn1.2xlarge',\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    disable_output_compression=True,\n",
    "    \n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training-neuronx:2.1.2-neuronx-py310-sdk2.20.0-ubuntu20.04\",\n",
    "    env={\n",
    "        'NEURON_RT_NUM_CORES': str(tp_degree)\n",
    "    },\n",
    "    volume_size = 512,\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb655a-8d7e-4f9a-bf58-60e8431619d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## This will take ~9mins\n",
    "estimator.fit({\"checkpoint\": checkpoint_s3_uri})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16951708-093a-4d83-b176-70301d31b6a7",
   "metadata": {},
   "source": [
    "## 3) Deploy the compiled model to a SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46f381-7fdc-4907-b2e3-d0d87030b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "model_data=estimator.model_data\n",
    "print(f\"Model data: {model_data}\")\n",
    "\n",
    "instance_type=\"ml.inf2.xlarge\"\n",
    "num_workers=1\n",
    "\n",
    "print(f\"Instance type: {instance_type}. Num SM workers: {num_workers}\")\n",
    "pytorch_model = PyTorchModel(\n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-inference-neuronx:2.1.2-neuronx-py310-sdk2.20.0-ubuntu20.04\",\n",
    "    model_data=model_data,\n",
    "    role=role,    \n",
    "    name=name_from_base('orpo-llama3'),\n",
    "    sagemaker_session=sess,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    model_server_workers=num_workers,\n",
    "    framework_version=\"2.1.2\",\n",
    "    env = {\n",
    "        'SAGEMAKER_MODEL_SERVER_TIMEOUT': '3600',\n",
    "        'MAX_SEQ_LEN': str(max_seq_len),\n",
    "        'NEURON_RT_NUM_CORES': str(tp_degree)\n",
    "    }\n",
    "    # for production it is important to define vpc_config and use a vpc_endpoint\n",
    "    #vpc_config={\n",
    "    #    'Subnets': ['<SUBNET1>', '<SUBNET2>'],\n",
    "    #    'SecurityGroupIds': ['<SECURITYGROUP1>', '<DEFAULTSECURITYGROUP>']\n",
    "    #}\n",
    ")\n",
    "pytorch_model._is_compiled_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57008b-1140-4c11-8750-055ad1fd6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    model_data_download_timeout=3600, # it takes some time to download all the artifacts and load the model\n",
    "    container_startup_health_check_timeout=1800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a9da6-dbea-4217-9c04-bbe9e5b42171",
   "metadata": {},
   "source": [
    "## 4) Run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777315c4-b4fe-4d7f-837d-f972076fb803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f97ca-1f62-48a4-a2d6-e24408c15fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#prompt = \"Explain the traditional techniques involved in cultivating a Bonsai tree.\"\n",
    "prompt = \"June and Julia live 1 mile apart. It takes June 4 minutes to ride her bike directly to Julia's house. At the same rate, how many minutes would it take June to ride the 3.5 miles from her own house to Bernard's house?\"\n",
    "t=time.time()\n",
    "pred = predictor.predict({\"prompt\": prompt})\n",
    "elapsed = (time.time()-t)*1000\n",
    "print(f\"Elapsed time: {elapsed}ms\")\n",
    "print(f\"Pred: {pred['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906fe91-5aec-42cc-a3fa-e625cf7456ea",
   "metadata": {},
   "source": [
    "Done! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
