{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2e683d-6131-463b-93a5-678f98064760",
   "metadata": {},
   "source": [
    "# LLM Domain Adaptation with ORPO, AWS Trainium and AWS Inferentia2\n",
    "\n",
    "Language models are incredibly powerful, but adapting them to specific tasks can be challenging. Traditional approaches involve two separate stages: first, supervised fine-tuning to align the model with the desired domain, and then a preference alignment step to increase the likelihood of desirable outputs and reduce undesirable ones.\n",
    "\n",
    "However, this two-stage process has limitations. While supervised fine-tuning is effective at domain adaptation, it can inadvertently increase the chances of generating both preferred and undesired responses.\n",
    "\n",
    "To address this issue, techniques like reinforcement learning with human feedback (RLHF) or direct preference optimization (DPO) are often employed for preference alignment. These methods aim to sculpt the model's outputs towards desired responses and away from rejected ones. However, they require a separate reference model, adding computational complexity.\n",
    "\n",
    "[Odds-Ratio Predictive Ordering (ORPO)](https://arxiv.org/abs/2403.07691) offers an elegant solution by combining supervised fine-tuning and preference alignment into a single objective function. It modifies the standard language modeling loss by incorporating an odds ratio term that weakly penalizes rejected responses while strongly rewarding preferred ones.\n",
    "\n",
    "In essence, ORPO streamlines the adaptation process by simultaneously fine-tuning the model to the target domain and aligning its preferences towards desired outputs â€“ all within a single training objective. This unified approach simplifies the workflow and reduces computational overhead compared to traditional multi-stage methods.\n",
    "\n",
    "----\n",
    "This is the first notebook out of two parts. In this notebook you download a public dataset (with questions, chosen answers and rejected answers) and upload it to S3. Then, you kick-off a SageMaker training Job that will execute a given training script (defined inline in this notebook) to do model alignment using ORPO with a Llama3.2 1B params and [HF Optium Neuron](https://huggingface.co/docs/optimum-neuron/index). This job will be accelerated by AWS Trainium for better performance and lower costs. In the end, you have a fine-tuned Llama3.2 model adapted with boundaries (expressed by the provided dataset). In the second notebook, you deploy the resulting model and run experimentations.\n",
    "\n",
    "**SageMaker Studio**: Jupyter Lab  \n",
    "**Kernel**: Python3  \n",
    "\n",
    "This exercise is divide into 2 parts:\n",
    " - **Data prep + model alignment**\n",
    " - Model deployment + tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa7e29-9000-449d-84ae-98572bce24c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U datasets s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c677ca-5932-400b-b314-5d546e9fa6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "\n",
    "## ATTENTION: Copy your HF Access token to the following variable, if the assertion fails\n",
    "HF_TOKEN=\"\"\n",
    "tok_file = os.path.join(os.environ['HOME'], '.hf_token')\n",
    "if os.path.isfile(tok_file): HF_TOKEN=open(tok_file, 'r').read().strip()    \n",
    "assert HF_TOKEN != \"\", \" >>> Go to your HF account and get an access token. Set HF_TOKEN to your token if you want to define your own cache repo\"\n",
    "\n",
    "region=\"us-west-2\"\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(boto_session=boto_session)\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")\n",
    "print(f\"HF Token found? {HF_TOKEN != ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7a61bd-e33a-44c5-b2df-506206c74d19",
   "metadata": {},
   "source": [
    "## 1) Prepare the dataset\n",
    "In this step we'll download a dataset from HF, get only a slice of it and then upload to S3. The samples we'll use is a collection of 25+ different datasets. With this dataaset, we can create a baseline for a super agent, capable of executing tasks like:\n",
    "\n",
    "-  [capybara-preferences](https://huggingface.co/datasets/argilla/Capybara-Preferences): instruction-following with multi-turn conversations\n",
    "-  [distillabel-orca](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs): reasoning process, step-by-step\n",
    "-  [ultrafeedback](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned): instruction-following, truthfulness, honesty and helpfulness\n",
    "\n",
    "And much more. Check more details here: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k/viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1308338-c747-497d-8ef5-b05958a5ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "num_samples=1024\n",
    "idx = 1\n",
    "\n",
    "train_dataset = datasets.load_dataset(\"mlabonne/orpo-dpo-mix-40k\", split=\"all\")\n",
    "# Remove toxicity\n",
    "# Source: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k?row=0#toxicity\n",
    "train_dataset = train_dataset.filter(\n",
    "    lambda r: not r[\"source\"] in [ \"toxic-dpo-v0.2\" ]\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(num_samples))\n",
    "df = train_dataset.to_pandas()\n",
    "print(f\"Mixed datasets: {list(df.source.unique())}\")\n",
    "train_path = f\"s3://{bucket}/datasets/orpo-dpo-mix-40k/train\"\n",
    "train_dataset.save_to_disk(train_path)\n",
    "print(f\"Train path: {train_path}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10db188-2532-430b-adf7-dc8a8baab3d6",
   "metadata": {},
   "source": [
    "#### This is an example of what the pre-processing code will do to prepare each sample to be shared with the model\n",
    "\n",
    "#### Chosen - Original Sample\n",
    "```json\n",
    "[{'content': '8801155689192/9 =?\\nOnly respond with math and no words.',\n",
    "  'role': 'user'},\n",
    " {'content': '8801155689192 / 9 = 977906187688', 'role': 'assistant'}]\n",
    "```\n",
    "#### Chat template applied\n",
    "```\n",
    "<|im_start|>user\n",
    "8801155689192/9 =?\n",
    "Only respond with math and no words.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "8801155689192 / 9 = 977906187688<|im_end|>\n",
    "```\n",
    "#### Rejected - Original Sample\n",
    "```json\n",
    "[{'content': '8801155689192/9 =?\\nOnly respond with math and no words.',\n",
    "  'role': 'user'},\n",
    " {'content': '88.8904838532465', 'role': 'assistant'}]\n",
    "```\n",
    "#### Chat template applied\n",
    "```\n",
    "<|im_start|>user\n",
    "8801155689192/9 =?\n",
    "Only respond with math and no words.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "88.8904838532465<|im_end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff579e52-c216-47d5-8af3-ef0078229d7e",
   "metadata": {},
   "source": [
    "## 2) Create training artifacts\n",
    "### 2.1) Dependencies descriptor\n",
    "Installing the libraries listed in this file will be the first thing SageMaker will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b9a94-e476-476c-8414-7b7e596b0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "--extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "optimum-neuron==0.0.26\n",
    "trl==0.11.4\n",
    "peft==0.13.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b5ff1c-7508-461e-aaea-dffd1cbf8dd4",
   "metadata": {},
   "source": [
    "### 2.1) Training script\n",
    "Please note the arguments passed to this script are the **hyperparameters** defined in the SageMaker Estimator (next section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ed787-988e-429c-b383-9e8b4304688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/train.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model\n",
    ")\n",
    "from trl import setup_chat_format\n",
    "from huggingface_hub import login\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from optimum.neuron import NeuronORPOConfig, NeuronORPOTrainer\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--num_samples\", type=int, default=32)\n",
    "    parser.add_argument(\"--max_seq_len\", type=int, default=256)\n",
    "    parser.add_argument(\"--max_prompt_len\", type=int, default=128)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--tp_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--pp_size\", type=int, default=1)\n",
    "    \n",
    "    parser.add_argument(\"--model_id\", type=str, required=True)\n",
    "    parser.add_argument(\"--zero_1\", type=bool, default=True)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
    "    parser.add_argument(\"--bf16\", type=bool, default=True)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\", \"output\"))\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\", \"model\"))\n",
    "\n",
    "    parser.add_argument(\"--training_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\", None))\n",
    "\n",
    "    parser.add_argument(\"--hf_token\", type=str, default=None)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    if not args.hf_token is None and len(args.hf_token) > 0:\n",
    "        print(\"HF token defined. Logging in...\")\n",
    "        login(token=args.hf_token)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_id)\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
    "    model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "    \n",
    "    print(f\"Loading dataset...\")\n",
    "    train_dataset = load_from_disk(args.training_dir)\n",
    "\n",
    "    def format_chat_template(row):\n",
    "        row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
    "        row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
    "        return row\n",
    "        \n",
    "    train_dataset = train_dataset.map(\n",
    "        format_chat_template,\n",
    "        num_proc=os.cpu_count(),\n",
    "    )\n",
    "    \n",
    "    training_args = NeuronORPOConfig(\n",
    "        max_length=args.max_seq_len,\n",
    "        max_prompt_length=args.max_prompt_len,\n",
    "        beta=0.1,\n",
    "        \n",
    "        zero_1=args.zero_1,\n",
    "        bf16=args.bf16,\n",
    "        tensor_parallel_size=args.tp_size,\n",
    "        pipeline_parallel_size=args.pp_size,\n",
    "        \n",
    "        #eval_strategy=\"epoch\",\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "\n",
    "        num_train_epochs=args.epochs,\n",
    "        output_dir=args.output_data_dir,\n",
    "        overwrite_output_dir=True,\n",
    "\n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        #per_device_eval_batch_size=args.eval_batch_size,\n",
    "\n",
    "        gradient_accumulation_steps=1,\n",
    "        #eval_accumulation_steps=1,\n",
    "\n",
    "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        save_steps=50,\n",
    "        max_grad_norm=1,\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=1,\n",
    "        remove_unused_columns=False,\n",
    "        hub_token=args.hf_token\n",
    "    )\n",
    "\n",
    "    trainer = NeuronORPOTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        peft_config=peft_config,\n",
    "        train_dataset=train_dataset\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a9f770-d31d-4584-90cf-d3b7f7703840",
   "metadata": {},
   "source": [
    "## 3) Kick-off the fine-tuning job\n",
    "First we create a [SageMaker Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) with all the parameters we need to launch a training job.\n",
    "\n",
    "It takes ~27 mins to fine-tune a Llama3.2-1B model using 1 trn1.2xlarge. This time includes 1/ code initialization; 2/ dependencies installing; 3/ model fine-tuning; 4/ trained model uploading to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01f425-b037-4e18-b736-2ce4f03f6f35",
   "metadata": {},
   "source": [
    "### 3.1) SageMaker estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531b481-6f94-458f-9016-467cad8faf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "tp_degree=1\n",
    "batch_size=1\n",
    "max_seq_len=512\n",
    "max_prompt_length=256\n",
    "\n",
    "# ATTENTION: To use llama2 you need to pass HF_TOKEN of an account\n",
    "# with permission to download Llama2 weights, otherwise the training will fail\n",
    "ARENA_MAX,model_id=128,\"meta-llama/Llama-3.2-1B\"\n",
    "## For some reason, if you get throttled by HF, uncomment the following line\n",
    "#ARENA_MAX,model_id=128,\"unsloth/Llama-3.2-1B\"\n",
    "\n",
    "# the default cache repo points to a public / read-only cache\n",
    "# You can point it to your own repo, but make sure you properly defined the HF token in the HF_TOKEN (above)\n",
    "CUSTOM_CACHE_REPO=\"aws-neuron/optimum-neuron-cache\"\n",
    "\n",
    "instance_type='ml.trn1.2xlarge'\n",
    "\n",
    "hyperparameters={\n",
    "    \"epochs\": 3,\n",
    "    \"num_samples\": 1024,\n",
    "    \"max_seq_len\": max_seq_len,\n",
    "    \"max_prompt_len\": max_prompt_length,\n",
    "    \"tp_size\": tp_degree,\n",
    "    \"pp_size\": 1,\n",
    "    \"zero_1\": True,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"bf16\": True,\n",
    "    \"eval_batch_size\": batch_size,\n",
    "    \"train_batch_size\": batch_size,\n",
    "    \"model_id\": model_id\n",
    "}\n",
    "\n",
    "if HF_TOKEN and len(HF_TOKEN) > 3:\n",
    "    hyperparameters[\"hf_token\"]= HF_TOKEN\n",
    "\n",
    "print(f\"Instance type: {instance_type}\\nHyperparameters: {hyperparameters}\")\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,    \n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    #input_mode='FastFile', # makes FS read-only\n",
    "    disable_output_compression=True,\n",
    "    \n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training-neuronx:2.1.2-neuronx-py310-sdk2.20.0-ubuntu20.04\",\n",
    "    \n",
    "    volume_size = 512,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    },\n",
    "    environment={\n",
    "        # Uncomment the following line to precompile the cache files\n",
    "        #\"RUN_NEURON_PARALLEL_COMPILE\": \"1\",\n",
    "        \"OMP_NUM_THREADS\": \"1\",\n",
    "        \"FI_EFA_FORK_SAFE\": \"1\",\n",
    "        \"FI_EFA_USE_DEVICE_RDMA\": \"1\",\n",
    "        \"FI_PROVIDER\": \"efa\",\n",
    "        \"XLA_DOWNCAST_BF16\": \"1\",\n",
    "        \"NEURON_FUSE_SOFTMAX\": \"1\",\n",
    "        \"NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS\": \"5\",\n",
    "        \n",
    "        \"NEURON_RT_STOCHASTIC_ROUNDING_EN\": \"1\",\n",
    "        \"CUSTOM_CACHE_REPO\": CUSTOM_CACHE_REPO,\n",
    "        \"MALLOC_ARENA_MAX\": str(ARENA_MAX), # required to avoid OOM\n",
    "        \"NEURON_CC_FLAGS\": \"--retry_failed_compilation --distribution-strategy=llm-training --enable-saturate-infinity\"\n",
    "    },\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"train_loss\", \"Regex\": \"'loss.:\\S*(.*?),\"},\n",
    "        {\"Name\": \"it_per_sec\", \"Regex\": \",\\S*(.*?)it.s.\"},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7bf4e-250d-451c-9e75-bc996029cd4e",
   "metadata": {},
   "source": [
    "### 3.2) Launch a SageMaker training job\n",
    "This will take ~25mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592bccb9-9181-4423-898c-7222f33f0f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "estimator.fit({\"train\": train_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde07a3-4547-43f3-8026-6a1addcbf850",
   "metadata": {},
   "source": [
    "#### Training cost\n",
    "To compute the training cost, check the \"Price per Hour\" here and calculate: https://aws.amazon.com/sagemaker/pricing/\n",
    "\n",
    "**EXAMPLE**\n",
    "```\n",
    "Billable seconds: 1425 (~23mins)\n",
    "Instance: ml.trn1.2xlarge\n",
    "Price per hour (26 Nov 2024): $1.54531\n",
    "Training cost: 1425 / 60.0 / 60.0 * 1.54531 = $0.61\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a72322c-49e1-47c5-8d47-4b8f190ffc0a",
   "metadata": {},
   "source": [
    "### 3.3) Save some parameters for the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b7e31-fd23-4f56-906c-993be404721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_job_name.txt\", \"w\") as f:\n",
    "    f.write(estimator._current_job_name)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(region)\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c042a-35a6-4300-9ee1-1b8fcfacc72b",
   "metadata": {},
   "source": [
    "**[Now, go to the next notebook: Deploy the fine-tuned model](02_DeployModel.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
